{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7706969e",
   "metadata": {},
   "source": [
    "1. Provide an example of the concepts of Prior, Posterior, and Likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c52aed",
   "metadata": {},
   "source": [
    "Let's consider a scenario where we want to predict the likelihood of a patient having a certain disease based on their symptoms.\n",
    "\n",
    "Prior probability: The prior probability is the probability of having the disease before considering the patient's symptoms. Let's say that the prior probability of having the disease in the general population is 1%, which means that 1 in 100 people have the disease.\n",
    "\n",
    "Likelihood probability: The likelihood probability is the probability of observing the patient's symptoms given that they have the disease. Let's say that the likelihood of a patient having a certain symptom if they have the disease is 80%, which means that if 100 patients have the disease, 80 of them would have this symptom.\n",
    "\n",
    "Posterior probability: The posterior probability is the probability of having the disease given the patient's symptoms. We can calculate the posterior probability using Bayes' theorem:\n",
    "\n",
    "Posterior probability = (Likelihood probability * Prior probability) / Evidence\n",
    "\n",
    "where Evidence is the probability of observing the patient's symptoms, regardless of whether they have the disease or not.\n",
    "\n",
    "Let's say that the patient has the symptom we mentioned earlier. Using Bayes' theorem, we can calculate the posterior probability of the patient having the disease as:\n",
    "\n",
    "Posterior probability = (0.80 * 0.01) / (Probability of having the symptom in the general population)\n",
    "\n",
    "If the probability of having the symptom in the general population is 10%, then the posterior probability would be 8%, which means that the patient has an 8% chance of having the disease given their symptom.\n",
    "\n",
    "In this example, the prior probability is the probability of having the disease before considering the patient's symptoms, the likelihood probability is the probability of observing the patient's symptom given that they have the disease, and the posterior probability is the probability of having the disease given the patient's symptom."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353ca2dd",
   "metadata": {},
   "source": [
    "2. What role does Bayes theorem play in the concept learning principle?\n",
    "\n",
    "Bayes theorem plays a fundamental role in the concept learning principle, which is a key principle in machine learning and artificial intelligence. The concept learning principle states that an agent should use Bayes theorem to update its beliefs about a concept or hypothesis based on new evidence or observations.\n",
    "\n",
    "In concept learning, an agent learns a concept or hypothesis based on a set of training examples, which are labeled with their corresponding class or category. The agent then uses this concept or hypothesis to classify new, unseen examples.\n",
    "\n",
    "Bayes theorem provides a principled way to update the agent's beliefs about the concept or hypothesis based on new evidence or observations. It allows the agent to compute the posterior probability of the concept or hypothesis given the new evidence, by combining the prior probability of the concept or hypothesis with the likelihood of the evidence given the concept or hypothesis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadcf48d",
   "metadata": {},
   "source": [
    "3. Offer an example of how the Nave Bayes classifier is used in real life\n",
    "\n",
    "One common application of the Naive Bayes classifier in real life is in spam filtering. The classifier can be trained on a large dataset of emails, where each email is labeled as either spam or non-spam (also known as ham). The classifier then uses the words and phrases in the email to calculate the probability that the email is spam or non-spam, based on the Naive Bayes algorithm.\n",
    "\n",
    "For example, suppose an email contains the words \"money\", \"free\", and \"click here\". Based on its training, the classifier calculates the likelihood that an email containing these words is spam. If the likelihood is high, the classifier labels the email as spam and moves it to the spam folder.\n",
    "\n",
    "The Naive Bayes classifier is particularly effective in spam filtering because it is able to handle high-dimensional data, such as the text of an email, with relatively few training examples. It is also computationally efficient, making it suitable for processing large volumes of email in real-time.\n",
    "\n",
    "Other real-life applications of the Naive Bayes classifier include sentiment analysis, document classification, and medical diagnosis, among others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a93f6b9",
   "metadata": {},
   "source": [
    "4. Can the Nave Bayes classifier be used on continuous numeric data? If so, how can you go about doing it?\n",
    "\n",
    "some modifications to the algorithm used for discrete data.\n",
    "\n",
    "One approach is to assume a probability distribution for the continuous data, such as a normal or Gaussian distribution. The Naive Bayes classifier can then be used to calculate the likelihood of each feature value given the class label, using the probability density function of the assumed distribution. The prior probabilities can be estimated from the training data as usual.\n",
    "\n",
    "Another approach is to discretize the continuous data into categorical or ordinal values, and then apply the standard Naive Bayes algorithm for discrete data. This can be done using techniques such as binning or clustering, which group similar values into categories.\n",
    "\n",
    "It is important to note that the choice of approach for handling continuous data can have a significant impact on the performance of the Naive Bayes classifier. The assumptions made about the probability distribution can affect the accuracy of the classifier, and discretization can lead to loss of information and introduce bias.\n",
    "\n",
    "Overall, while the Naive Bayes classifier is typically used for discrete data, it can be adapted to handle continuous data with some modifications and careful consideration of the underlying assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee23412",
   "metadata": {},
   "source": [
    "5. What are Bayesian Belief Networks, and how do they work? What are their applications? Are they capable of resolving a wide range of issues?\n",
    "\n",
    "Bayesian Belief Networks (BBNs) are probabilistic graphical models that represent the relationships between variables and their probabilistic dependencies using a directed acyclic graph (DAG). The nodes of the graph represent the variables, and the edges represent the dependencies between them. The BBN model provides a way to represent and reason about uncertainty and probabilistic relationships between variables.\n",
    "\n",
    "BBNs work by applying Bayes' theorem and the chain rule of probability to propagate probabilities and update beliefs based on evidence. The network structure enables efficient computation of joint probabilities of all variables, even in the presence of missing or uncertain data.\n",
    "\n",
    "BBNs have a wide range of applications, including decision making, diagnosis, prediction, risk assessment, and data analysis. For example, in medical diagnosis, a BBN can be used to model the probabilistic dependencies between symptoms, diseases, and test results, and to update the probabilities of different diagnoses based on new evidence.\n",
    "\n",
    "BBNs are capable of resolving a wide range of issues, but they do have some limitations. One limitation is that they can become computationally expensive and complex for large networks or complex dependencies between variables. Another limitation is that they require accurate prior probabilities and can be sensitive to biases or errors in the input data. However, with careful design and implementation, BBNs can be a powerful tool for modeling and reasoning about complex systems under uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036e78b4",
   "metadata": {},
   "source": [
    "\n",
    "6. Passengers are checked in an airport screening system to see if there is an intruder. Let I be the random variable that indicates whether someone is an intruder I = 1) or not I = 0), and A be the variable that indicates alarm I = 0). If an intruder is detected with probability P(A = 1|I = 1) = 0.98 and a non-intruder is detected with probability P(A = 1|I = 0) = 0.001, an alarm will be triggered, implying the error factor. The likelihood of an intruder in the passenger population is P(I = 1) = 0.00001. What are the chances that an alarm would be triggered when an individual is actually an intruder?\n",
    "\n",
    "\n",
    "\n",
    "We are given:\n",
    "\n",
    "P(A = 1|I = 1) = 0.98: the probability of an alarm being triggered given that there is an intruder.\n",
    "P(A = 1|I = 0) = 0.001: the probability of an alarm being triggered given that there is no intruder.\n",
    "P(I = 1) = 0.00001: the prior probability of there being an intruder.\n",
    "We want to find P(I = 1|A = 1), the probability of there being an intruder given that an alarm was triggered.\n",
    "\n",
    "Using Bayes' theorem, we can calculate this as:\n",
    "\n",
    "P(I = 1|A = 1) = P(A = 1|I = 1) * P(I = 1) / P(A = 1)\n",
    "\n",
    "To find P(A = 1), we can use the law of total probability:\n",
    "\n",
    "P(A = 1) = P(A = 1|I = 1) * P(I = 1) + P(A = 1|I = 0) * P(I = 0)\n",
    "\n",
    "Substituting the given probabilities, we get:\n",
    "\n",
    "P(A = 1) = (0.98 * 0.00001) + (0.001 * 0.99999) = 0.0010898\n",
    "\n",
    "Now we can calculate P(I = 1|A = 1) using Bayes' theorem:\n",
    "\n",
    "P(I = 1|A = 1) = 0.98 * 0.00001 / 0.0010898 = 0.00008976\n",
    "\n",
    "Therefore, the probability of an individual being an intruder given that an alarm was triggered is approximately 0.00008976 or 0.008976%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7becdc59",
   "metadata": {},
   "source": [
    "7. An antibiotic resistance test (random variable T) has 1% false positives (i.e., 1% of those who are not immune to an antibiotic display a positive result in the test) and 5% false negatives (i.e., 1% of those who are not resistant to an antibiotic show a positive result in the test) (i.e. 5 percent of those actually resistant to an antibiotic test negative). Assume that 2% of those who were screened were antibiotic-resistant. Calculate the likelihood that a person who tests positive is actually immune (random variable D).\n",
    "\n",
    "\n",
    "\n",
    "We are given:\n",
    "\n",
    "P(T = 1|D = 0) = 0.01: the probability of a false positive, i.e., the probability of testing positive given that the person is not actually immune to the antibiotic.\n",
    "P(T = 0|D = 1) = 0.05: the probability of a false negative, i.e., the probability of testing negative given that the person is actually immune to the antibiotic.\n",
    "P(D = 1) = 0.02: the prior probability of a person being immune to the antibiotic.\n",
    "We want to find P(D = 1|T = 1), the probability of a person being immune to the antibiotic given that they tested positive.\n",
    "\n",
    "Using Bayes' theorem, we can calculate this as:\n",
    "\n",
    "P(D = 1|T = 1) = P(T = 1|D = 1) * P(D = 1) / P(T = 1)\n",
    "\n",
    "To find P(T = 1), we can use the law of total probability:\n",
    "\n",
    "P(T = 1) = P(T = 1|D = 0) * P(D = 0) + P(T = 1|D = 1) * P(D = 1)\n",
    "\n",
    "Substituting the given probabilities, we get:\n",
    "\n",
    "P(T = 1) = (0.01 * 0.98) + (0.05 * 0.02) = 0.0112\n",
    "\n",
    "Now we can calculate P(D = 1|T = 1) using Bayes' theorem:\n",
    "\n",
    "P(D = 1|T = 1) = 0.98 * 0.02 / 0.0112 = 0.1741\n",
    "\n",
    "Therefore, the likelihood that a person who tests positive is actually immune to the antibiotic is approximately 0.1741 or 17.41%.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84c7bd7",
   "metadata": {},
   "source": [
    "\n",
    "8. In order to prepare for the test, a student knows that there will be one question in the exam that is either form A, B, or C. The chances of getting an A, B, or C on the exam are 30 percent, 20%, and 50 percent, respectively. During the planning, the student solved 9 of 10 type A problems, 2 of 10 type B problems, and 6 of 10 type C problems.\n",
    "\n",
    "\n",
    "1. What is the likelihood that the student can solve the exam problem?\n",
    "\n",
    "2. Given the student's solution, what is the likelihood that the problem was of form A?\n",
    "\n",
    "To calculate the likelihood that the student can solve the exam problem, we can use Bayes' theorem. Let A be the event that the problem is of form A, B be the event that the problem is of form B, and C be the event that the problem is of form C. Let S be the event that the student can solve the problem. We want to calculate P(S), the probability that the student can solve the problem. Using the law of total probability, we have:\n",
    "P(S) = P(S|A)P(A) + P(S|B)P(B) + P(S|C)P(C)\n",
    "\n",
    "where P(S|A) is the probability that the student can solve a type A problem, P(S|B) is the probability that the student can solve a type B problem, and P(S|C) is the probability that the student can solve a type C problem.\n",
    "\n",
    "From the problem statement, we know that P(A) = 0.3, P(B) = 0.2, and P(C) = 0.5. We also know that the student solved 9 of 10 type A problems, 2 of 10 type B problems, and 6 of 10 type C problems. Therefore:\n",
    "\n",
    "P(S|A) = 9/10 = 0.9\n",
    "P(S|B) = 2/10 = 0.2\n",
    "P(S|C) = 6/10 = 0.6\n",
    "\n",
    "Substituting these values into the formula, we get:\n",
    "\n",
    "P(S) = 0.9(0.3) + 0.2(0.2) + 0.6(0.5) = 0.57\n",
    "\n",
    "Therefore, the likelihood that the student can solve the exam problem is 0.57.\n",
    "\n",
    "Given the student's solution, we want to calculate the likelihood that the problem was of form A. Using Bayes' theorem, we have:\n",
    "P(A|S) = P(S|A)P(A) / P(S)\n",
    "\n",
    "where P(S|A) is the probability that the student can solve a type A problem (which we calculated in part 1), P(A) is the prior probability that the problem is of form A (which is given in the problem statement), and P(S) is the probability that the student can solve the problem (which we calculated in part 1).\n",
    "\n",
    "Substituting the values we know, we get:\n",
    "\n",
    "P(A|S) = 0.9(0.3) / 0.57 = 0.474\n",
    "\n",
    "Therefore, the likelihood that the problem was of form A, given the student's solution, is 0.474."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa8ea0d",
   "metadata": {},
   "source": [
    "9. A bank installs a CCTV system to track and photograph incoming customers. Despite the constant influx of customers, we divide the timeline into 5 minute bins. There may be a customer coming into the bank with a 5% chance in each 5-minute time period, or there may be no customer (again, for simplicity, we assume that either there is 1 customer or none, not the case of multiple customers). If there is a client, the CCTV will detect them with a 99 percent probability. If there is no customer, the camera can take a false photograph with a 10% chance of detecting movement from other objects.\n",
    "\n",
    "  1. How many customers come into the bank on a daily basis (10 hours)?\n",
    "\n",
    "  2. On a daily basis, how many fake photographs (photographs taken when there is no customer) and how many missed photographs (photographs taken when there is a customer) are there?\n",
    "\n",
    "  3. Explain likelihood that there is a customer if there is a photograph?\n",
    "  \n",
    "  \n",
    "There are 72 five-minute intervals in a 10-hour day. Assuming a 5% chance of a customer arriving in each interval, the expected number of customers in a day would be:\n",
    "Expected number of customers = 72 * 0.05 = 3.6\n",
    "\n",
    "Therefore, on average, 3-4 customers are expected to come into the bank on a daily basis.\n",
    "\n",
    "The probability of a false photograph being taken when there is no customer is 10%, and the probability of a customer being missed is 1% (since the CCTV has a 99% detection rate). Therefore, the expected number of false photographs and missed photographs in a day would be:\n",
    "Expected number of false photographs = 72 * 0.10 = 7.2\n",
    "\n",
    "Expected number of missed photographs = 3.6 * 0.01 = 0.036\n",
    "\n",
    "Therefore, on average, there would be around 7-8 false photographs and less than 1 missed photograph in a day.\n",
    "\n",
    "If there is a photograph, we want to calculate the probability that there is a customer present. This can be calculated using Bayes' theorem:\n",
    "P(customer present | photograph taken) = P(photograph taken | customer present) * P(customer present) / P(photograph taken)\n",
    "\n",
    "From the problem statement, we have:\n",
    "\n",
    "P(photograph taken | customer present) = 0.99\n",
    "P(customer present) = 0.05\n",
    "P(photograph taken | no customer present) = 0.10\n",
    "\n",
    "We can calculate P(photograph taken) using the law of total probability:\n",
    "\n",
    "P(photograph taken) = P(photograph taken | customer present) * P(customer present) + P(photograph taken | no customer present) * P(no customer present)\n",
    "\n",
    "P(photograph taken) = 0.99 * 0.05 + 0.10 * 0.95 = 0.1045\n",
    "\n",
    "Plugging these values into Bayes' theorem, we get:\n",
    "\n",
    "P(customer present | photograph taken) = 0.99 * 0.05 / 0.1045 = 0.474\n",
    "\n",
    "Therefore, if a photograph is taken, there is a 47.4% likelihood that there is a customer present.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f15ba6e",
   "metadata": {},
   "source": [
    "\n",
    "10. Create the conditional probability table associated with the node Won Toss in the Bayesian Belief network to represent the conditional independence assumptions of the Nave Bayes classifier for the match winning prediction problem in Section 6.4.4.\n",
    "\n",
    "Assuming that the possible outcomes for the Won Toss node are \"Yes\" and \"No,\" and the features used in the Naive Bayes classifier are \"Team,\" \"Opponent,\" \"Venue,\" and \"Weather,\" the conditional probability table would be as follows:\n",
    "\n",
    "Won Toss\tP(Won Toss)\n",
    "Yes\t         0.5\n",
    "No           0.5\n",
    "The conditional probabilities for each feature, given the outcome of the Won Toss, would be:\n",
    "\n",
    "\n",
    "| Won Toss | Team = India | Team = Australia | Opponent = England | Opponent = South Africa | Venue = Home | Venue = Away | Weather = Sunny | Weather = Cloudy |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| Yes | 0.4 | 0.3 | 0.2 | 0.1 | 0.6 | 0.4 | 0.7 | 0.3 |\n",
    "| No | 0.3 | 0.4 | 0.1 | 0.2 | 0.4 | 0.6 | 0.4 | 0.6 |\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671be775",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
