{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e00a35a7",
   "metadata": {},
   "source": [
    "1.\tWhat is the definition of a target function? In the sense of a real-life example, express the target function. How is a target function's fitness assessed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6758b7",
   "metadata": {},
   "source": [
    "1. In the context of machine learning and statistical modeling, a target function (also known as a response variable or dependent variable) is the output or outcome variable that we want to predict or model based on a set of input or independent variables.\n",
    "2. For example, suppose we want to predict the price of a house based on various features such as the size of the house, the number of bedrooms and bathrooms, the location, and so on. In this case, the price of the house would be the target function.\n",
    "3. The fitness of a target function is typically assessed using a loss or objective function. The loss function measures the difference between the predicted values and the true values of the target function, given a set of input variables. The goal is to minimize the loss function, which essentially means reducing the difference between the predicted and true values as much as possible.\n",
    "4. For example, in linear regression, the loss function is typically the mean squared error (MSE), which measures the average squared difference between the predicted values and the true values of the target function. In classification problems, different types of loss functions such as cross-entropy or hinge loss are used, depending on the nature of the problem.\n",
    "5. Overall, the target function and its fitness assessment are critical components of many machine learning and statistical modeling applications, as they allow us to make predictions and decisions based on data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224f6652",
   "metadata": {},
   "source": [
    "\n",
    "2. What are predictive models, and how do they work? What are descriptive types, and how do you use them? Examples of both types of models should be provided. Distinguish between these two forms of models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d3eacc",
   "metadata": {},
   "source": [
    "Predictive models and descriptive models are two types of models used in machine learning and data analysis.\n",
    " A predictive model is a statistical or machine learning model that is trained on data to make predictions about future events or outcomes. The model is trained on a set of input variables and a target variable (i.e., the variable we want to predict), and it learns the patterns and relationships between the input variables and the target variable. Once the model is trained, it can be used to make predictions on new data.\n",
    "For example, a predictive model could be used to predict whether a customer is likely to purchase a product based on their past purchase history, demographics, and other relevant variables.\n",
    "On the other hand, a descriptive model is a statistical or machine learning model that is used to summarize or describe a dataset. These models do not make predictions but rather help us understand the patterns and relationships in the data.\n",
    "For example, a descriptive model could be used to identify the most common demographic characteristics of customers who have purchased a product in the past.\n",
    "The main difference between predictive and descriptive models is their purpose. Predictive models are designed to make predictions about future events or outcomes, while descriptive models are designed to summarize or describe a dataset.\n",
    "Examples of predictive models include:\n",
    "1. Linear regression: a statistical model that predicts a numerical target variable based on one or more input variables.\n",
    "2. Random forest: a machine learning model that can be used for classification or regression tasks by combining multiple decision trees.\n",
    "3. Recurrent neural network: a type of neural network that can be used for sequential data, such as time series data or text data.\n",
    "Examples of descriptive models include:\n",
    "4. Cluster analysis: a statistical technique that groups similar observations into clusters based on their characteristics.\n",
    "5. Principal component analysis: a statistical technique that reduces the dimensionality of a dataset while retaining the most important information.\n",
    "6. Decision tree: a machine learning model that visualizes the decision-making process based on a set of rules or conditions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ed3777",
   "metadata": {},
   "source": [
    "3. Describe the method of assessing a classification model's efficiency in detail. Describe the various measurement parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ec9bba",
   "metadata": {},
   "source": [
    "1. Accuracy: The accuracy is the proportion of correctly classified instances to the total number of instances in the dataset. It is given by:\n",
    "Accuracy = (True Positives + True Negatives) / (True Positives + False Positives + True Negatives + False Negatives)\n",
    "True Positives (TP) are the number of correctly predicted positive instances, False Positives (FP) are the number of incorrectly predicted positive instances, True Negatives (TN) are the number of correctly predicted negative instances, and False Negatives (FN) are the number of incorrectly predicted negative instances.\n",
    "2. Precision: Precision measures the proportion of correctly predicted positive instances to the total number of predicted positive instances. It is given by:\n",
    "Precision = True Positives / (True Positives + False Positives)\n",
    "3. Recall: Recall measures the proportion of correctly predicted positive instances to the total number of actual positive instances in the dataset. It is given by:\n",
    "Recall = True Positives / (True Positives + False Negatives)\n",
    "4. F1 score: The F1 score is the harmonic mean of the precision and recall. It balances both precision and recall and is a good measure of a classifier's overall performance. It is given by:\n",
    "F1 score = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "5. Area under the ROC curve (AUC): The AUC is a performance metric that evaluates the classifier's ability to distinguish between positive and negative instances across different thresholds. It measures the area under the Receiver Operating Characteristic (ROC) curve, which plots the true positive rate (sensitivity) against the false positive rate (1-specificity) at different classification thresholds.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a7e9e7",
   "metadata": {},
   "source": [
    "4. \n",
    "i. In the sense of machine learning models, what is underfitting? What is the most common reason for underfitting?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d89aa54",
   "metadata": {},
   "source": [
    "Underfitting is a common problem in machine learning models where the model is unable to capture the underlying patterns in the data and performs poorly on both the training and test datasets. In other words, the model is too simple and fails to learn the complex relationships between the input variables and the target variable.\n",
    "\n",
    "The most common reason for underfitting is a lack of model complexity or capacity. If the model is too simple, it may not be able to capture the complexity of the data, and as a result, the model will not perform well on either the training or test dataset. This can happen when the model is not sufficiently flexible to learn the underlying patterns in the data.\n",
    "\n",
    "Another reason for underfitting is a lack of data. If the model does not have enough data to learn the patterns in the data, it may not be able to generalize well to new data. In this case, the model may perform well on the training dataset but poorly on the test dataset.\n",
    "\n",
    "Underfitting can also occur if the features used to train the model are not relevant to the target variable. In this case, the model will not be able to learn the underlying patterns in the data and will perform poorly on both the training and test datasets.\n",
    "\n",
    "To address underfitting, we can try increasing the model complexity by adding more layers or neurons to the model or using a more complex model architecture. We can also try adding more relevant features to the dataset or collecting more data. It's essential to strike a balance between model complexity and the amount of data available to ensure that the model is not overfitting or underfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb4ae56",
   "metadata": {},
   "source": [
    "ii. What does it mean to overfit? When is it going to happen?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b135199",
   "metadata": {},
   "source": [
    "Overfitting is a common problem in machine learning models where the model performs very well on the training dataset but poorly on the test dataset. In other words, the model has learned the noise and patterns specific to the training dataset instead of the underlying patterns that generalize well to new data.\n",
    "\n",
    "Overfitting happens when the model is too complex and has too many parameters compared to the amount of data available. When the model has a large number of parameters, it can fit the training dataset perfectly, including the noise and outliers in the data, leading to overfitting. However, the model's ability to generalize to new data is compromised because it has learned to fit the idiosyncrasies of the training data rather than the underlying patterns.\n",
    "\n",
    "Another common cause of overfitting is when the model is trained for too many epochs or with too many iterations. This can cause the model to continue to optimize the training data's fit, leading to overfitting and poor performance on new data.\n",
    "\n",
    "Overfitting can also occur when the training dataset is not representative of the population we want to generalize the model to. In this case, the model may learn to fit the patterns specific to the training dataset and fail to generalize to new data.\n",
    "\n",
    "To address overfitting, we can use regularization techniques such as L1, L2 regularization, or dropout to reduce the model's complexity and prevent it from learning noise and patterns specific to the training dataset. We can also use cross-validation to evaluate the model's performance on a validation dataset and select the best model based on its performance on the validation dataset. Another approach is to increase the size of the training dataset or generate synthetic data to ensure that the model learns the underlying patterns and generalizes well to new data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62fd4b8",
   "metadata": {},
   "source": [
    "iii. In the sense of model fitting, explain the bias-variance trade-off."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1720f4",
   "metadata": {},
   "source": [
    "The bias-variance trade-off is a fundamental concept in machine learning that describes the relationship between a model's ability to fit the training data and its ability to generalize to new data.\n",
    "\n",
    "Bias refers to the difference between the expected predictions of the model and the true values of the target variable. A model with high bias may be too simple to capture the underlying patterns in the data and may underfit the data.\n",
    "\n",
    "Variance, on the other hand, refers to the variability of the model's predictions for different training datasets. A model with high variance may be too complex and may overfit the data by learning noise and patterns specific to the training dataset.\n",
    "\n",
    "The bias-variance trade-off arises because increasing the complexity of the model typically reduces its bias but increases its variance, while decreasing the model's complexity typically reduces its variance but increases its bias. The goal is to find the right balance between bias and variance to obtain a model that performs well on both the training dataset and the new data.\n",
    "\n",
    "To achieve this balance, we can use regularization techniques such as L1, L2 regularization, or dropout to reduce the model's complexity and prevent it from overfitting the data. We can also use ensemble methods such as bagging, boosting, or stacking to combine multiple models and reduce the variance of the predictions.\n",
    "\n",
    "In summary, the bias-variance trade-off is a critical concept in machine learning that highlights the need to balance the model's complexity to achieve optimal performance on both the training dataset and new data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ecd74d",
   "metadata": {},
   "source": [
    "6.\tIs it possible to boost the efficiency of a learning model? If so, please clarify how."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0533ee",
   "metadata": {},
   "source": [
    "Yes, it is possible to boost the efficiency of a learning model in several ways. Here are some approaches to consider:\n",
    "7. Improve the quality and quantity of data: More data, especially diverse and high-quality data, can help the model to learn more complex patterns and generalize better to new data.\n",
    "\n",
    "8. Feature engineering: Feature engineering involves transforming the raw data into features that the model can use to learn patterns. Good feature engineering can lead to better performance in the model.\n",
    "9. Hyperparameter tuning: Hyperparameters are parameters that cannot be learned directly from the data, such as the learning rate, regularization strength, and model architecture. Tuning these hyperparameters can help to optimize the model's performance.\n",
    "\n",
    "10. Regularization: Regularization techniques such as L1, L2 regularization, or dropout can help to reduce overfitting and improve the model's ability to generalize to new data.\n",
    "11. Ensemble methods: Ensemble methods such as bagging, boosting, or stacking can help to combine multiple models and improve the model's performance by reducing variance and improving accuracy.\n",
    "12. Transfer learning: Transfer learning involves using a pre-trained model on a large dataset and fine-tuning it on a smaller dataset. This approach can help to boost the efficiency of the model, especially when the smaller dataset is similar to the pre-trained model's dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b565d08",
   "metadata": {},
   "source": [
    "7.\tHow would you rate an unsupervised learning model's success? What are the most common success indicators for an unsupervised learning model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d0bd96",
   "metadata": {},
   "source": [
    "The success of an unsupervised learning model can be measured in several ways. Here are some of the most common success indicators for an unsupervised learning model:\n",
    "1. Clustering performance: Clustering is one of the most common unsupervised learning tasks. In clustering, the goal is to group similar data points together. One way to measure the success of a clustering model is to use metrics such as silhouette score or adjusted Rand index.\n",
    "2. Dimensionality reduction performance: Dimensionality reduction techniques such as principal component analysis (PCA) or t-distributed stochastic neighbor embedding (t-SNE) are often used in unsupervised learning. The success of a dimensionality reduction model can be evaluated by assessing how well it preserves the structure of the data.\n",
    "3. Anomaly detection performance: Anomaly detection is another common unsupervised learning task. In anomaly detection, the goal is to identify unusual data points. The success of an anomaly detection model can be evaluated by measuring its accuracy, precision, recall, or F1 score.\n",
    "4. Reconstruction performance: Autoencoders are a popular type of unsupervised learning model that can be used for tasks such as image or audio reconstruction. The success of an autoencoder model can be evaluated by measuring how well it can reconstruct the original input from the encoded representation.\n",
    "5. Generative performance: Generative models such as generative adversarial networks (GANs) or variational autoencoders (VAEs) can be used to generate new data that is similar to the training data. The success of a generative model can be evaluated by assessing the quality of the generated samples using metrics such as Inception score or Fréchet Inception distance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a42700f",
   "metadata": {},
   "source": [
    "6.\tIs it possible to use a classification model for numerical data or a regression model for categorical data with a classification model? Explain your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a7bcf7",
   "metadata": {},
   "source": [
    "1.\tNo, it is not appropriate to use a classification model for numerical data or a regression model for categorical data.\n",
    "\n",
    "2.\tClassification models are designed to predict categorical variables, where the outcome is a discrete set of classes or categories. These models are trained on labeled data where the outcome variable is categorical. The output of a classification model is a probability distribution over the possible categories, indicating the likelihood of the input belonging to each category.\n",
    "\n",
    "3.\tOn the other hand, regression models are designed to predict continuous variables, where the outcome is a continuous value or a range of values. These models are trained on labeled data where the outcome variable is numerical. The output of a regression model is a numerical value that represents the predicted value of the input.\n",
    "\n",
    "4.\tUsing a classification model for numerical data or a regression model for categorical data will likely lead to poor performance because the model is not designed for that type of data. For example, if we try to use a classification model to predict a numerical value, the model will not be able to provide a meaningful probability distribution over possible values. Similarly, if we try to use a regression model to predict a categorical variable, the model will not be able to provide a meaningful numerical value that represents the predicted category.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764ade5b",
   "metadata": {},
   "source": [
    "8.\tDescribe the predictive modeling method for numerical values. What distinguishes it from categorical predictive modeling?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0e281b",
   "metadata": {},
   "source": [
    "Predictive modeling for numerical values involves using a regression model to predict a continuous numerical value. The goal is to build a model that can accurately predict the numerical output based on a set of input features.\n",
    "The process of predictive modeling for numerical values typically involves the following steps:\n",
    "1. Data pre-processing: This involves cleaning the data, handling missing values, and transforming the data if necessary.\n",
    "2. Feature engineering: This involves selecting or creating relevant features that are likely to be useful for the model.\n",
    "3. Model selection: This involves choosing an appropriate regression model based on the nature of the problem and the data.\n",
    "4. Model training: This involves training the selected model on the labeled data to learn the relationship between the input features and the numerical output.\n",
    "5. Model evaluation: This involves evaluating the performance of the model on a separate validation set or through cross-validation to ensure that it is not overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00e1ef5",
   "metadata": {},
   "source": [
    "9. The following data were collected when using a classification model to predict the malignancy of a group of patients' tumors: 1. Accurate estimates – 15 cancerous, 75 benign \n",
    "2. Wrong predictions – 3 cancerous, 7 benign\n",
    "\n",
    "Determine the model's error rate, Kappa value, sensitivity, precision, and F-measure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203e3976",
   "metadata": {},
   "source": [
    "1.\tError rate: The error rate is the proportion of incorrect predictions out of all predictions made. In this case, there are a total of 100 predictions made, and 10 of them are wrong. Thus, the error rate is:\n",
    "Error rate = (3+7)/100 = 0.1 or 10%\n",
    "2.Kappa value: The kappa value is a measure of agreement between the predictions made by the model and the actual outcomes. It takes into account the possibility of random agreement. The kappa value is calculated as follows:\n",
    "a. Calculate the observed agreement (OA), which is the proportion of cases where the model's prediction matches the actual outcome:\n",
    "OA=(75+15)/100=0.9 or 90%\n",
    "b. Calculate the expected agreement (EA), which is the proportion of cases where we would expect the model to make the correct prediction by chance:\n",
    "EA=[(15+3)/100]*[(15+7)/100]+[(75+3)/100]*[(75+7)/100]\n",
    "    =(0.18*0.22)+(0.78*0.82)]\n",
    "    =0.65\n",
    "c. Calculate the kappa value using the formula:\n",
    "Kappa = (OA - EA) / (1 - EA) = (0.9 - 0.65) / (1 - 0.65) = 0.44\n",
    "1. Sensitivity: Sensitivity is a measure of how well the model identifies positive cases. It is calculated as the proportion of true positives out of all actual positive cases. In this case, there are 18 actual cancerous cases, and the model correctly identifies 15 of them. Thus, the sensitivity is:\n",
    "Sensitivity = 15/18 = 0.83 or 83.3%\n",
    "2. Precision: Precision is a measure of how well the model predicts positive cases. It is calculated as the proportion of true positives out of all predicted positive cases. In this case, the model predicts 18 positive cases, of which 15 are true positives. Thus, the precision is:\n",
    "Precision = 15/18 = 0.83 or 83.3%\n",
    "3. F-measure: The F-measure is a combined measure of precision and recall (sensitivity), which takes into account both false positives and false negatives. It is calculated as the harmonic mean of precision and recall:\n",
    "F-measure = 2 x (Precision x Sensitivity) / (Precision + Sensitivity) = 2 x (0.83 x 0.83) / (0.83 + 0.83) = 0.83\n",
    "Thus, the F-measure is 0.83.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a903778e",
   "metadata": {},
   "source": [
    "10. Make quick notes on:\n",
    "1. The process of holding out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c46e6eb",
   "metadata": {},
   "source": [
    "The process of holding out, also known as holdout validation, is a method used to evaluate the performance of a predictive model. It involves randomly dividing the available data into two parts: a training set and a validation set.\n",
    "\n",
    "The training set is used to fit the model, while the validation set is used to assess the model's performance. The model is trained on the training set, and then the validation set is used to make predictions. The predictions are compared to the actual outcomes, and various metrics are calculated to evaluate the model's performance.\n",
    "\n",
    "Holdout validation is commonly used when the available data is limited, as it allows for an unbiased estimate of the model's performance on new, unseen data. The validation set serves as a proxy for new, unseen data that the model will encounter in the real world.\n",
    "\n",
    "Holdout validation can also be used to compare the performance of different models or different parameter settings for the same model. By evaluating each model or parameter setting on the same validation set, it is possible to determine which one performs best.\n",
    "\n",
    "One drawback of holdout validation is that it can be sensitive to the way in which the data is split into training and validation sets. In particular, if the data is not representative of the population the model will encounter in the real world, the performance on the validation set may not accurately reflect the performance on new, unseen data. Cross-validation is one technique that can be used to mitigate this problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89959f0a",
   "metadata": {},
   "source": [
    "2.\tCross-validation by tenfold\n",
    "2.In tenfold cross-validation, the data is divided into ten subsets, with each subset containing an equal number of samples. The model is then trained on nine of the subsets and tested on the remaining subset. This process is repeated ten times, with each subset being used as the test set once. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68517100",
   "metadata": {},
   "source": [
    "3. Adjusting the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5c4810",
   "metadata": {},
   "source": [
    "In machine learning, adjusting the parameters of a model is an essential part of the model-building process. Parameters are the settings or values that control how the model works and how it makes predictions. Examples of model parameters include the learning rate in a neural network or the number of trees in a random forest.\n",
    "\n",
    "Adjusting the parameters of a model can significantly impact its performance. For example, increasing the number of trees in a random forest model may improve its accuracy, but it may also increase its training time and memory usage. Similarly, decreasing the learning rate in a neural network may lead to better convergence and better accuracy, but it may also increase the training time.\n",
    "\n",
    "The process of adjusting the parameters of a model is often called hyperparameter tuning. Hyperparameters are the parameters of a model that are set before training begins, and they are not learned during the training process. Examples of hyperparameters include the number of hidden layers in a neural network or the regularization strength in a linear regression model.\n",
    "\n",
    "Hyperparameter tuning is typically done using a search algorithm that tries different combinations of hyperparameters and evaluates the model's performance on a validation set. The goal is to find the combination of hyperparameters that gives the best performance on the validation set. One common approach is grid search, which involves evaluating the model's performance for every possible combination of hyperparameters in a predefined search space. Other approaches include random search, Bayesian optimization, and genetic algorithms.\n",
    "\n",
    "Hyperparameter tuning can be a time-consuming and computationally expensive process, particularly for complex models with many hyperparameters. However, it is an essential part of building an accurate and reliable model, and it can make a significant difference in the model's performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32aa847c",
   "metadata": {},
   "source": [
    "11. Define the following terms: \n",
    "1. Purity vs. Silhouette width\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b91e24",
   "metadata": {},
   "source": [
    "Purity and silhouette width are two different evaluation metrics used in clustering analysis to measure the quality of a clustering result.\n",
    "\n",
    "Purity is a metric used to evaluate the quality of a clustering result when the true labels of the data points are known. It measures the percentage of correctly assigned data points in each cluster. Purity ranges from 0 to 1, where a value of 1 indicates perfect clustering. Purity is simple to compute and interpret, but it assumes that the true labels are known, which is often not the case in practice.\n",
    "\n",
    "Silhouette width is a metric used to evaluate the quality of a clustering result when the true labels of the data points are unknown.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a3db45",
   "metadata": {},
   "source": [
    "3.\tBoosting vs. Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ce6cf0",
   "metadata": {},
   "source": [
    "1.Boosting and bagging are two different techniques used in ensemble learning, where multiple models are combined to improve the overall predictive performance.\n",
    "4. Bagging (short for bootstrap aggregating) is a technique that involves training multiple independent models on different subsets of the training data, where each model is trained on a randomly sampled subset of the data with replacement. The final prediction is made by aggregating the predictions of all the individual models. Bagging can reduce overfitting and improve generalization by reducing the variance of the model, especially for unstable models like decision trees.\n",
    "\n",
    "2. Boosting, on the other hand, is a technique that involves training multiple models sequentially, where each subsequent model is trained on the misclassified samples from the previous model. The final prediction is made by aggregating the predictions of all the individual models, weighted by their respective performance. Boosting can improve the accuracy of the model by reducing both bias and variance, especially for weak models like decision stumps.\n",
    "\n",
    "3. The key difference between boosting and bagging is the way they select the training data and weight the predictions. Bagging selects random subsets of the training data and weights the predictions equally, while boosting selects the misclassified samples and weights the predictions based on the model's performance. Bagging reduces variance by averaging multiple independent models, while boosting reduces bias by emphasizing misclassified samples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2757b5",
   "metadata": {},
   "source": [
    "\n",
    "4.\tThe eager learner vs. the lazy learner\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cc463c",
   "metadata": {},
   "source": [
    "1.The terms \"eager learner\" and \"lazy learner\" refer to two different types of machine learning algorithms based on their approach to learning.\n",
    "\n",
    "2.An eager learner is a machine learning algorithm that eagerly builds a model based on the entire training data before receiving new data for prediction. Eager learners are also known as \"eager classifiers\" or \"eager models.\" These algorithms typically involve the computation of a complex model during the training phase, which can be computationally expensive. Once trained, they can quickly classify new instances using the precomputed model. Examples of eager learners include Decision Trees, Artificial Neural Networks, and Support Vector Machines.\n",
    "\n",
    "3.On the other hand, a lazy learner is a machine learning algorithm that postpones the computation of a model until a prediction is required. Lazy learners are also known as \"lazy classifiers\" or \"lazy models.\" These algorithms store the training data in memory and only calculate a model when a new instance needs to be classified. Lazy learners are computationally cheap during the training phase but can be slower during prediction as the model needs to be calculated for each new instance. Examples of lazy learners include k-Nearest Neighbor and Locally Weighted Regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d9e8ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
