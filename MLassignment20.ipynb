{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93de2188",
   "metadata": {},
   "source": [
    "1. What is the underlying concept of Support Vector Machines?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3678aa",
   "metadata": {},
   "source": [
    "Support Vector Machines (SVMs) are a type of machine learning algorithm used for classification and regression analysis. The underlying concept of SVMs is to find the best hyperplane that separates the data points into different classes while maximizing the margin between the hyperplane and the closest data points. The hyperplane is defined as the linear boundary that separates the two classes in the feature space. The goal of SVM is to find the hyperplane that maximizes the margin between the two classes while minimizing the classification error. In addition, SVMs can also use a kernel function to map the input data into a higher-dimensional space to make the problem more easily separable. SVMs are often used in real-world applications such as text classification, image recognition, and bioinformatics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c458e8",
   "metadata": {},
   "source": [
    "2. What is the concept of a support vector?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb17b925",
   "metadata": {},
   "source": [
    "\n",
    "In the context of Support Vector Machines (SVMs), a support vector is a data point that lies closest to the hyperplane, and hence has the most influence on the location and orientation of the hyperplane. The distance between the hyperplane and the support vectors is called the margin, and the SVM algorithm aims to maximize this margin while correctly classifying the training data. The support vectors are the data points that define the margin, as any small change in their position will affect the location and orientation of the hyperplane. The support vectors play a crucial role in SVMs as they determine the optimal hyperplane that separates the two classes of data. Additionally, the number of support vectors is typically much smaller than the total number of data points, which allows SVMs to handle high-dimensional data efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999ed690",
   "metadata": {},
   "source": [
    "3. When using SVMs, why is it necessary to scale the inputs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2e4505",
   "metadata": {},
   "source": [
    "When using Support Vector Machines (SVMs), it is necessary to scale the inputs to ensure that all features contribute equally to the distance measurements. SVMs attempt to maximize the margin between the support vectors and the decision boundary. The margin is the perpendicular distance between the support vectors and the decision boundary. The SVM algorithm is sensitive to the scale of the input data because distance calculations between data points rely on the magnitude of each feature. \n",
    "\n",
    "If the input features are not scaled, then features with larger magnitudes may dominate the distance calculation, resulting in suboptimal classification performance. Scaling the inputs to a common scale ensures that each feature contributes equally to the distance calculation, making the SVM less biased towards specific features. \n",
    "\n",
    "Thus, scaling the input features can help the SVM converge faster and result in more accurate and reliable classification models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c344d78",
   "metadata": {},
   "source": [
    "4. When an SVM classifier classifies a case, can it output a confidence score? What about a percentage chance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe06a24",
   "metadata": {},
   "source": [
    "Yes, an SVM classifier can output a confidence score for its predictions. This score is often referred to as the \"distance to the hyperplane,\" which is the distance between the predicted point and the decision boundary. The larger the distance, the more confident the classifier is in its prediction. However, this distance is not directly interpretable as a percentage chance, as it does not represent a probability in the same way as other classifiers like logistic regression or naive Bayes. Instead, it reflects the margin of separation between the two classes, with larger margins indicating more confident predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ea35a9",
   "metadata": {},
   "source": [
    "5. Should you train a model on a training set with millions of instances and hundreds of features using the primal or dual form of the SVM problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1086bd87",
   "metadata": {},
   "source": [
    "For training a model on a large dataset with many features, the dual form of the SVM problem is generally preferred because it can be more computationally efficient than the primal form. The dual form of the problem involves solving a quadratic optimization problem involving only the training examples' dot products, which can be computed using a precomputed kernel matrix. This can make training much faster than the primal form, which involves solving a linear optimization problem with potentially millions of variables. However, for smaller datasets with fewer features, the primal form may be more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482d668e",
   "metadata": {},
   "source": [
    "\n",
    "6. Let's say you've used an RBF kernel to train an SVM classifier, but it appears to underfit the training collection. Is it better to raise or lower (gamma)? What about the letter C?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f9db54",
   "metadata": {},
   "source": [
    "When an SVM classifier trained with an RBF kernel underfits the training data, it means that the model is too simple and does not capture the complexity of the data. In such a situation, we need to increase the model's complexity by adjusting the hyperparameters gamma and C.\n",
    "\n",
    "Gamma determines the influence of a single training example on the classification boundary. A lower gamma value implies a higher influence, resulting in a more complex and wiggly decision boundary, whereas a higher gamma value reduces the influence of individual training examples, leading to a smoother and simpler boundary. Therefore, to address the underfitting issue, we need to reduce gamma to increase the influence of individual training examples.\n",
    "\n",
    "C is the regularization parameter, which determines the tradeoff between maximizing the margin and minimizing the classification error. A higher C value indicates that the model should aim for a higher accuracy on the training set, while a lower C value allows for more misclassifications but a broader margin. To address underfitting, we can increase C to allow the model to fit the training data more tightly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9feb11b",
   "metadata": {},
   "source": [
    "\n",
    "7. To solve the soft margin linear SVM classifier problem with an off-the-shelf QP solver, how should the QP parameters (H, f, A, and b) be set?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef20a12f",
   "metadata": {},
   "source": [
    "To solve the soft margin linear SVM classifier problem using a QP solver, the QP parameters (H, f, A, and b) should be set as follows:\n",
    "\n",
    "1. H: It is a matrix that should be defined as an identity matrix multiplied by a small positive number (C) and the number of training instances. \n",
    "\n",
    "   `H = diag([C, C, ..., C])`\n",
    "\n",
    "2. f: It is a vector that should be set to all negative values (-1) as we are trying to minimize the cost function.\n",
    "\n",
    "   `f = -1 * ones(n_samples)`\n",
    "\n",
    "3. A: It is a matrix of the constraint coefficients. For the soft margin SVM classifier, there are two types of constraints: \n",
    "\n",
    "   a. The first constraint is that the labels (y) times the decision function (w^T x + b) must be greater than or equal to 1 for all training instances that are correctly classified. \n",
    "\n",
    "   b. The second constraint is that the decision function (w^T x + b) must be less than or equal to -1 for all training instances that are incorrectly classified. \n",
    "\n",
    "    `A = np.vstack((np.diag(y), np.diag(-y)))`\n",
    "\n",
    "4. b: It is a vector of the constraint values, which are all set to zero.\n",
    "\n",
    "   `b = np.zeros(n_samples)`\n",
    "\n",
    "After defining the QP parameters, they can be passed to an off-the-shelf QP solver to find the optimal values of the decision function parameters (w and b) that minimize the cost function subject to the constraint conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37f0170",
   "metadata": {},
   "source": [
    "8. On a linearly separable dataset, train a LinearSVC. Then, using the same dataset, train an SVC and an SGDClassifier. See if you can get them to make a model that is similar to yours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c024472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC accuracy: 1.0\n",
      "SVC accuracy: 1.0\n",
      "SGDClassifier accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate a linearly separable dataset\n",
    "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
    "                           n_clusters_per_class=1, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a LinearSVC\n",
    "linear_svc = LinearSVC()\n",
    "linear_svc.fit(X_train, y_train)\n",
    "\n",
    "# Train an SVC\n",
    "svc = SVC(kernel='linear')\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# Train an SGDClassifier\n",
    "sgd = SGDClassifier(loss='hinge', max_iter=1000)\n",
    "sgd.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the models on the testing set\n",
    "y_pred_linear_svc = linear_svc.predict(X_test)\n",
    "accuracy_linear_svc = accuracy_score(y_test, y_pred_linear_svc)\n",
    "\n",
    "y_pred_svc = svc.predict(X_test)\n",
    "accuracy_svc = accuracy_score(y_test, y_pred_svc)\n",
    "\n",
    "y_pred_sgd = sgd.predict(X_test)\n",
    "accuracy_sgd = accuracy_score(y_test, y_pred_sgd)\n",
    "\n",
    "print(\"LinearSVC accuracy:\", accuracy_linear_svc)\n",
    "print(\"SVC accuracy:\", accuracy_svc)\n",
    "print(\"SGDClassifier accuracy:\", accuracy_sgd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875a3df5",
   "metadata": {},
   "source": [
    "\n",
    "9. On the MNIST dataset, train an SVM classifier. You'll need to use one-versus-the-rest to assign all 10 digits because SVM classifiers are binary classifiers. To accelerate up the process, you might want to tune the hyperparameters using small validation sets. What level of precision can you achieve?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ce2f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\anaconda3\\lib\\site-packages\\sklearn\\datasets\\_openml.py:932: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# load the MNIST dataset\n",
    "mnist = fetch_openml('mnist_784')\n",
    "\n",
    "# split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(mnist.data, mnist.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# train an SVM classifier using a one-versus-the-rest strategy\n",
    "svm_clf = SVC(kernel='rbf', C=10, gamma=0.05, decision_function_shape='ovr')\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the test set\n",
    "y_pred = svm_clf.predict(X_test)\n",
    "\n",
    "# evaluate the classifier's accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70881bd0",
   "metadata": {},
   "source": [
    "10. On the California housing dataset, train an SVM regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff8def1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the California housing dataset\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an SVM regressor object\n",
    "svm_reg = SVR(kernel='linear')\n",
    "\n",
    "# Train the SVM regressor on the training data\n",
    "svm_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = svm_reg.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error of the predictions\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Print the mean squared error\n",
    "print(\"Mean squared error:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c350f70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
